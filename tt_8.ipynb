{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e5ec0e5-d21a-443e-b0b5-349e00fdf512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a51f3db2-3550-4891-adc0-de6f86509e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_tomato', 'valid_tomato']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data= 'data'\n",
    "os.listdir(my_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4edc6f00-a3e6-40ab-9f71-1dfa8fa898b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = my_data+'/train_tomato/'\n",
    "test_path = my_data+'/valid_tomato/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86bc25d3-5bd3-4686-8af2-1d6489763a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes =['Tomato___Bacterial_spot',\n",
    "          'Tomato___Early_blight',\n",
    "          'Tomato___healthy',\n",
    "          'Tomato___Late_blight',\n",
    "          'Tomato___Leaf_Mold',\n",
    "          'Tomato___Septoria_leaf_spot',\n",
    "          'Tomato___Spider_mites Two-spotted_spider_mite',\n",
    "          'Tomato___Target_Spot',\n",
    "          'Tomato___Tomato_mosaic_virus',\n",
    "          'Tomato___Tomato_Yellow_Leaf_Curl_Virus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ea0af1-633d-4b40-87d1-efa3a2ea691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************\n",
      "Class: Tomato___Bacterial_spot\n",
      "Max value: 255\n",
      "Min value: 0\n",
      "***************************\n",
      "Class: Tomato___Early_blight\n",
      "Max value: 255\n",
      "Min value: 0\n",
      "***************************\n",
      "Class: Tomato___healthy\n",
      "Max value: 255\n",
      "Min value: 0\n",
      "***************************\n",
      "Class: Tomato___Late_blight\n",
      "Max value: 255\n",
      "Min value: 0\n",
      "***************************\n",
      "Class: Tomato___Leaf_Mold\n",
      "Max value: 255\n",
      "Min value: 0\n",
      "***************************\n",
      "Class: Tomato___Septoria_leaf_spot\n",
      "Max value: 255\n",
      "Min value: 0\n",
      "***************************\n",
      "Class: Tomato___Spider_mites Two-spotted_spider_mite\n",
      "Max value: 255\n",
      "Min value: 0\n",
      "***************************\n",
      "Class: Tomato___Target_Spot\n",
      "Max value: 255\n",
      "Min value: 0\n",
      "***************************\n",
      "Class: Tomato___Tomato_mosaic_virus\n",
      "Max value: 255\n",
      "Min value: 0\n",
      "***************************\n",
      "Class: Tomato___Tomato_Yellow_Leaf_Curl_Virus\n",
      "Max value: 255\n",
      "Min value: 0\n",
      "***************************\n"
     ]
    }
   ],
   "source": [
    "path='data/train_tomato'\n",
    "for tomato_class in classes:\n",
    "    class_path = os.path.join(path, tomato_class)\n",
    "    max_values = []\n",
    "    min_values = []\n",
    "    for image in os.listdir(class_path):\n",
    "        img = imread(os.path.join(class_path, image))\n",
    "        max_val = img.max()\n",
    "        min_val = img.min()\n",
    "        max_values.append(max_val)\n",
    "        min_values.append(min_val)\n",
    "    max_class_val = max(max_values)\n",
    "    min_class_val = min(min_values)\n",
    "    print(\"***************************\")\n",
    "    print(f\"Class: {tomato_class}\")\n",
    "    print(f\"Max value: {max_class_val}\")\n",
    "    print(f\"Min value: {min_class_val}\")\n",
    "\n",
    "print(\"***************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3aaaf29-d298-4245-9268-12f469b3dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f8317b-1cd1-40e2-a912-a1115aae64f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbb83f95-d5a7-4c10-8ef3-493ea4862a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18345 files belonging to 10 classes.\n",
      "Using 14676 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  path,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=input_shape,\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e581f0f5-5277-4bbf-abc3-6a788b632ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18345 files belonging to 10 classes.\n",
      "Using 3669 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  path,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=input_shape,\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fe7df9b-94e6-4c57-a1d1-10dfd54d5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c7c32f6-a965-4de3-a369-7c7e6c673f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(lambda x,y: (x/255, y))\n",
    "val_ds = val_ds.map(lambda x,y: (x/255, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ce53582-c5fa-40fc-9044-92a575085d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, labels_batch = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0858deec-ef62-4b0c-a2e3-6f78e57f5a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image = image_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e98c528e-85e7-4ecf-bbc4-8c2310e55779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.7882353\n"
     ]
    }
   ],
   "source": [
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4cb64f1-867f-4d90-b908-13ec041237b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout,BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0b3ff18-6377-4c53-a738-d48abbcee760",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape =  (256,256,3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5343b797-bc6f-451a-86dd-266893e3766c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(classes)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a9fd66b-fe6d-4420-9342-c211226587b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a94f3f6b-b492-45ce-8a66-189a3b7f1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9afa6ac-57f9-4509-ab94-d55535ab5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fc7bc7f-f074-4aa3-9571-e8498c4bbdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7a92893-f9a8-48f9-9435-193feae71a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Funct  (None, 1280)             2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               655872    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                8224      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,053,738\n",
      "Trainable params: 795,754\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MobileNetV2_model = Sequential()\n",
    "\n",
    "pretrained_model = MobileNetV2(include_top=False,\n",
    "                   input_shape=(256, 256, 3),\n",
    "                   pooling='avg',\n",
    "                   weights='imagenet')\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "MobileNetV2_model.add(pretrained_model)\n",
    "MobileNetV2_model.add(Flatten())\n",
    "# ANN\n",
    "MobileNetV2_model.add(Dense(512, activation='relu'))\n",
    "MobileNetV2_model.add(Dense(256, activation='relu'))\n",
    "MobileNetV2_model.add(Dropout(0.2))\n",
    "\n",
    "MobileNetV2_model.add(Dense(32, activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "MobileNetV2_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "MobileNetV2_model.compile(optimizer=Adam(learning_rate=0.003),\n",
    "                          loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "MobileNetV2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a1f9d13-0e2f-4a7a-b508-494c71bedef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "230/230 [==============================] - 65s 226ms/step - loss: 1.0024 - accuracy: 0.6466 - val_loss: 0.4823 - val_accuracy: 0.8384\n",
      "Epoch 2/50\n",
      "230/230 [==============================] - 48s 204ms/step - loss: 0.3860 - accuracy: 0.8679 - val_loss: 0.3848 - val_accuracy: 0.8744\n",
      "Epoch 3/50\n",
      "230/230 [==============================] - 48s 205ms/step - loss: 0.2551 - accuracy: 0.9143 - val_loss: 0.3592 - val_accuracy: 0.8795\n",
      "Epoch 4/50\n",
      "230/230 [==============================] - 47s 205ms/step - loss: 0.2045 - accuracy: 0.9313 - val_loss: 0.3008 - val_accuracy: 0.9060\n",
      "Epoch 5/50\n",
      "230/230 [==============================] - 47s 206ms/step - loss: 0.1849 - accuracy: 0.9385 - val_loss: 0.2339 - val_accuracy: 0.9207\n",
      "Epoch 6/50\n",
      "230/230 [==============================] - 48s 207ms/step - loss: 0.1451 - accuracy: 0.9537 - val_loss: 0.2272 - val_accuracy: 0.9250\n",
      "Epoch 7/50\n",
      "230/230 [==============================] - 48s 206ms/step - loss: 0.1198 - accuracy: 0.9606 - val_loss: 0.2593 - val_accuracy: 0.9204\n",
      "Epoch 8/50\n",
      "230/230 [==============================] - 48s 208ms/step - loss: 0.1168 - accuracy: 0.9617 - val_loss: 0.3137 - val_accuracy: 0.9079\n",
      "Epoch 9/50\n",
      "230/230 [==============================] - 48s 208ms/step - loss: 0.0892 - accuracy: 0.9711 - val_loss: 0.2989 - val_accuracy: 0.9226\n",
      "Epoch 10/50\n",
      "230/230 [==============================] - 48s 207ms/step - loss: 0.0929 - accuracy: 0.9704 - val_loss: 0.2628 - val_accuracy: 0.9275\n",
      "Epoch 11/50\n",
      "230/230 [==============================] - 48s 209ms/step - loss: 0.0725 - accuracy: 0.9770 - val_loss: 0.2711 - val_accuracy: 0.9278\n",
      "Epoch 12/50\n",
      "230/230 [==============================] - 48s 210ms/step - loss: 0.0762 - accuracy: 0.9742 - val_loss: 0.3126 - val_accuracy: 0.9174\n",
      "Epoch 13/50\n",
      "230/230 [==============================] - 48s 210ms/step - loss: 0.0691 - accuracy: 0.9778 - val_loss: 0.2687 - val_accuracy: 0.9267\n",
      "Epoch 14/50\n",
      "230/230 [==============================] - 48s 209ms/step - loss: 0.0664 - accuracy: 0.9774 - val_loss: 0.3701 - val_accuracy: 0.9155\n",
      "Epoch 15/50\n",
      "230/230 [==============================] - 48s 209ms/step - loss: 0.0618 - accuracy: 0.9787 - val_loss: 0.2923 - val_accuracy: 0.9362\n",
      "Epoch 16/50\n",
      "230/230 [==============================] - 48s 209ms/step - loss: 0.0618 - accuracy: 0.9805 - val_loss: 0.3165 - val_accuracy: 0.9280\n",
      "Epoch 17/50\n",
      "230/230 [==============================] - 48s 209ms/step - loss: 0.0521 - accuracy: 0.9826 - val_loss: 0.2908 - val_accuracy: 0.9308\n",
      "Epoch 18/50\n",
      "230/230 [==============================] - 49s 210ms/step - loss: 0.0375 - accuracy: 0.9878 - val_loss: 0.3505 - val_accuracy: 0.9305\n",
      "Epoch 19/50\n",
      "230/230 [==============================] - 49s 211ms/step - loss: 0.0606 - accuracy: 0.9811 - val_loss: 0.2861 - val_accuracy: 0.9319\n",
      "Epoch 20/50\n",
      "230/230 [==============================] - 48s 210ms/step - loss: 0.0564 - accuracy: 0.9809 - val_loss: 0.3123 - val_accuracy: 0.9354\n",
      "Epoch 21/50\n",
      "230/230 [==============================] - 48s 208ms/step - loss: 0.0335 - accuracy: 0.9898 - val_loss: 0.3758 - val_accuracy: 0.9245\n",
      "Epoch 22/50\n",
      "230/230 [==============================] - 48s 207ms/step - loss: 0.0576 - accuracy: 0.9804 - val_loss: 0.3582 - val_accuracy: 0.9297\n",
      "Epoch 23/50\n",
      "230/230 [==============================] - 48s 209ms/step - loss: 0.0471 - accuracy: 0.9847 - val_loss: 0.3546 - val_accuracy: 0.9237\n",
      "Epoch 24/50\n",
      "230/230 [==============================] - 48s 208ms/step - loss: 0.0472 - accuracy: 0.9851 - val_loss: 0.3427 - val_accuracy: 0.9240\n",
      "Epoch 25/50\n",
      "230/230 [==============================] - 47s 205ms/step - loss: 0.0323 - accuracy: 0.9890 - val_loss: 0.3478 - val_accuracy: 0.9308\n",
      "Epoch 26/50\n",
      "230/230 [==============================] - 42s 181ms/step - loss: 0.0323 - accuracy: 0.9896 - val_loss: 0.5118 - val_accuracy: 0.9092\n"
     ]
    }
   ],
   "source": [
    "MobileNetV2_model_results = MobileNetV2_model.fit(train_ds, epochs=50,\n",
    "                             validation_data=val_ds,\n",
    "                             callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a4834e6-4369-41aa-8045-a769586ebb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 8s 143ms/step - loss: 0.5118 - accuracy: 0.9092\n",
      "Validation Loss: 0.5118280053138733\n",
      "Validation Accuracy: 0.9092395901679993\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "val_loss,val_acc = MobileNetV2_model.evaluate(val_ds)\n",
    "print(\"Validation Loss:\", val_loss)\n",
    "print(\"Validation Accuracy:\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9432a3e-95a7-45e9-94d2-29d268a66601",
   "metadata": {},
   "outputs": [],
   "source": [
    "MobileNetV2_model.save('tomato_disease_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df1097-f790-43e9-a988-571a2383ad24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
